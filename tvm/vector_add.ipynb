{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa1d9975-b7f6-49bf-a05c-ff323d5bde8b",
   "metadata": {},
   "source": [
    "## Vector Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65291237-44be-42c1-a8e2-6e7396dd1756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5246c789-8961-45af-bf43-7a0d38ae338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n = 100\n",
    "a = np.random.normal(size=n).astype(np.float32)\n",
    "b = np.random.normal(size=n).astype(np.float32)\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ad3783-f676-47af-abdc-147671ddd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_add(a, b, c):\n",
    "    for idx in range(n):\n",
    "        c[idx] = a[idx] + b[idx]\n",
    "\n",
    "d = np.empty(shape=n, dtype=np.float32)\n",
    "vector_add(a, b, d)\n",
    "np.testing.assert_array_equal(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f00ddf64-ad7a-454e-90d7-dbd7b6e9d2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tvm.te.tensor.Tensor, tvm.te.tensor.Tensor)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "\n",
    "\n",
    "def vector_add(n):\n",
    "    \"\"\"TVM expression for vector add\"\"\"\n",
    "    A = te.placeholder((n,), name='a')\n",
    "    B = te.placeholder((n,), name='b')\n",
    "    C = te.compute(A.shape, lambda i: A[i] + B[i], name='c')\n",
    "\n",
    "    return A, B, C\n",
    "\n",
    "A, B, C = vector_add(n)\n",
    "\n",
    "type(A), type(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f400ea87-1017-486e-bb88-e3e32d347dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tvm.te.tensor.PlaceholderOp, tvm.te.tensor.ComputeOp)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(A.op), type(C.op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6a09b-2daf-4ca5-bb8e-f2dfa0621f8f",
   "metadata": {},
   "source": [
    "## Creating the Schedule\n",
    "\n",
    "### Placeholder Creation\n",
    "\n",
    "When we create placeholders A and B using te.placeholder((n,), name='a') and te.placeholder((n,), name='b'), we're not allocating actual memory or providing real data values. Instead, we're creating symbolic representations of the input tensors.\n",
    "\n",
    "### Computation Definition \n",
    "The te.compute() function defines how the output C is computed symbolically, without actually performing the computation\n",
    "\n",
    "This is essential for creating the graph, on which we can do the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef2f1141-33ca-4d40-a15f-7168c7f222cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = te.create_schedule(C.op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c1ea3-3950-4cd4-9177-3bf8ed6dcd79",
   "metadata": {},
   "source": [
    "### Delayed Data Provision\n",
    "You don't need to have the actual input data available when defining the computation graph. This allows for a separation between the computation definition and its execution.\n",
    "### Runtime Data Feeding\n",
    "At runtime, when you're ready to execute the compiled function, you can provide the actual data for A and B.\n",
    "\n",
    "### Flexibility\n",
    "This approach allows the same compiled function to be used with different input data of the specified shape, without needing to redefine or recompile the computation.\n",
    "\n",
    "### What does schedule mean here?\n",
    "\n",
    "The concept of scheduling in TVM is about defining how the computation should be executed, optimized, and mapped to the target hardware. Let's break down what's happening and why we're passing C.op to create_schedule:\n",
    "\n",
    "#### What We're Scheduling\n",
    "When we create a schedule, we're essentially planning out:\n",
    "- Execution Order: How the operations should be ordered and nested\n",
    "- Memory Access Patterns: How data should be loaded and stored.\n",
    "- Parallelization: How to distribute work across threads or vectorize operations.\n",
    "- Hardware-Specific Optimizations: How to best utilize the target hardware's features.\n",
    "\n",
    "#### Why We Pass C.op to create_schedule\n",
    "\n",
    "We pass C.op to create_schedule for the following reasons:\n",
    "\n",
    "##### Root of Computation Graph\n",
    "C is our output tensor, and C.op represents the operation that produces C. By passing this to create_schedule, we're telling TVM to create a schedule for the entire computation graph that leads to C.\n",
    "\n",
    "##### Backwards Traversal\n",
    "TVM will start from the output operation (C.op) and work backwards through all the dependent operations (in this case, the operations on A and B) to create a complete schedule.\n",
    "\n",
    "##### Optimization Scope\n",
    "This approach allows TVM to consider the entire computation when making scheduling decisions, potentially leading to more globally optimal schedules.\n",
    "\n",
    "By passing C.op to create_schedule, we're initiating the process of planning how the entire computation (from inputs A and B to output C) should be executed, while keeping this execution plan separate from the definition of what is being computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab5400ac-75f8-4110-8574-543d6f70dfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tvm.te.schedule.Schedule, tvm.te.schedule.Stage)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s), type(s[C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23f10d49-4938-4184-b5c7-99ef7bba8fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#[version = \"0.0.5\"]\n",
       "@main = primfn(a_1: handle, b_1: handle, c_1: handle) -> ()\n",
       "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
       "  buffers = {a: Buffer(a_2: Pointer(float32), float32, [100], []),\n",
       "             b: Buffer(b_2: Pointer(float32), float32, [100], []),\n",
       "             c: Buffer(c_2: Pointer(float32), float32, [100], [])}\n",
       "  buffer_map = {a_1: a, b_1: b, c_1: c}\n",
       "  preflattened_buffer_map = {a_1: a_3: Buffer(a_2, float32, [100], []), b_1: b_3: Buffer(b_2, float32, [100], []), c_1: c_3: Buffer(c_2, float32, [100], [])} {\n",
       "  for (i: int32, 0, 100) {\n",
       "    c[i] = (a[i] + b[i])\n",
       "  }\n",
       "}\n",
       "\n",
       "#[metadata]\n",
       "{\n",
       "  \"root\": 1, \n",
       "  \"nodes\": [\n",
       "    {\n",
       "      \"type_key\": \"\"\n",
       "    }, \n",
       "    {\n",
       "      \"type_key\": \"Map\", \n",
       "      \"keys\": [\n",
       "        \"IntImm\"\n",
       "      ], \n",
       "      \"data\": [2]\n",
       "    }, \n",
       "    {\n",
       "      \"type_key\": \"Array\", \n",
       "      \"data\": [3, 4]\n",
       "    }, \n",
       "    {\n",
       "      \"type_key\": \"IntImm\", \n",
       "      \"attrs\": {\n",
       "        \"dtype\": \"bool\", \n",
       "        \"span\": \"0\", \n",
       "        \"value\": \"1\"\n",
       "      }\n",
       "    }, \n",
       "    {\n",
       "      \"type_key\": \"IntImm\", \n",
       "      \"attrs\": {\n",
       "        \"dtype\": \"bool\", \n",
       "        \"span\": \"0\", \n",
       "        \"value\": \"1\"\n",
       "      }\n",
       "    }\n",
       "  ], \n",
       "  \"b64ndarrays\": [], \n",
       "  \"attrs\": {\"tvm_version\": \"0.10.0\"}\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvm.lower(s, [A, B, C], simple_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa603e09-d16c-4769-b0b0-408445ef60c6",
   "metadata": {},
   "source": [
    "### Look at\n",
    "\n",
    "```\n",
    "oat32, [100], [])} {\n",
    "  for (i: int32, 0, 100) {\n",
    "    c[i] = (a[i] + b[i])\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95316198-7047-4767-b754-fae5c35e1f9f",
   "metadata": {},
   "source": [
    "### Compile the above code to a module, and then execute\n",
    "\n",
    "Once both computation and schedule are defined, we can compile them into an executable module with tvm.build. It accepts the same argument as tvm.lower. In fact, it first calls tvm.lower to generate the program and then compiles to machine codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "229a9685-2a44-4a61-b8c9-edcb57ffb312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.driver.build_module.OperatorModule"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = tvm.build(s, [A, B, C])\n",
    "type(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15cc548e-e923-4b1f-9af8-12f8dc1bd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abc(shape, constructor=None):\n",
    "    \"\"\"Return random a, b and empty c with the same shape.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    a = np.random.normal(size=shape).astype(np.float32)\n",
    "    b = np.random.normal(size=shape).astype(np.float32)\n",
    "    c = np.empty_like(a)\n",
    "\n",
    "    if constructor:\n",
    "        a, b, c = [constructor(x) for x in (a, b, c)]\n",
    "\n",
    "    return a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7cc6916-75b8-4795-82d4-065ecfcf9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = get_abc(100, tvm.nd.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb83b36-4861-4238-a33d-c5ca871f873e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.runtime.ndarray.NDArray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a6e735e-9ea9-4eca-9ff7-2f297556e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod(a, b, c)\n",
    "np.testing.assert_array_equal(a.asnumpy() + b.asnumpy(), c.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d278739-5a24-4636-88dd-a0997861d6fa",
   "metadata": {},
   "source": [
    "##### Note\n",
    "\n",
    "When we created the module,a and compiled it, we passed ```n``` as the fixed value 100.\n",
    "\n",
    "```\n",
    "n= 100\n",
    "\n",
    "def vector_add(n):\n",
    "    \"\"\"TVM expression for vector add\"\"\"\n",
    "    A = te.placeholder((n,), name='a')\n",
    "    B = te.placeholder((n,), name='b')\n",
    "    C = te.compute(A.shape, lambda i: A[i] + B[i], name='c')\n",
    "\n",
    "    return A, B, C\n",
    "\n",
    "A, B, C = vector_add(n)\n",
    "```\n",
    "\n",
    "As a result, the module can only accept the size we have created as the placeholder, the below gives an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52632537-cd8d-4bfd-9206-b83824874edd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  [bt] (3) 4   libtvm.dylib                        0x000000012f14f07e TVMFuncCall + 62\n  [bt] (2) 3   libtvm.dylib                        0x000000012f16af12 tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>>::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) + 418\n  [bt] (1) 2   libtvm.dylib                        0x000000012d3ad3f9 tvm::runtime::detail::LogFatal::Entry::Finalize() + 89\n  [bt] (0) 1   libtvm.dylib                        0x000000012f16c038 tvm::runtime::Backtrace() + 24\n  File \"/Users/runner/work/tlcpack/tlcpack/tvm/src/runtime/library_module.cc\", line 80\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n\n  Check failed: ret == 0 (-1 vs. 0) : Assert fail: (100 == int32(arg.a.shape[0])), Argument arg.a.shape[0] has an unsatisfied constraint: (100 == int32(arg.a.shape[0]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a, b, c \u001b[38;5;241m=\u001b[39m get_abc(\u001b[38;5;241m200\u001b[39m, tvm\u001b[38;5;241m.\u001b[39mnd\u001b[38;5;241m.\u001b[39marray)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/tvm_scripts/d2l-tvm/tvm/venv/lib/python3.9/site-packages/tvm/runtime/module.py:198\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_entry:\n\u001b[0;32m--> 198\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_func(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi:331\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.PackedFuncBase.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi:262\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi:251\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.FuncCall3\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./base.pxi:181\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.CHECK_CALL\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  [bt] (3) 4   libtvm.dylib                        0x000000012f14f07e TVMFuncCall + 62\n  [bt] (2) 3   libtvm.dylib                        0x000000012f16af12 tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>>::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) + 418\n  [bt] (1) 2   libtvm.dylib                        0x000000012d3ad3f9 tvm::runtime::detail::LogFatal::Entry::Finalize() + 89\n  [bt] (0) 1   libtvm.dylib                        0x000000012f16c038 tvm::runtime::Backtrace() + 24\n  File \"/Users/runner/work/tlcpack/tlcpack/tvm/src/runtime/library_module.cc\", line 80\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n\n  Check failed: ret == 0 (-1 vs. 0) : Assert fail: (100 == int32(arg.a.shape[0])), Argument arg.a.shape[0] has an unsatisfied constraint: (100 == int32(arg.a.shape[0]))"
     ]
    }
   ],
   "source": [
    "a, b, c = get_abc(200, tvm.nd.array)\n",
    "mod(a, b, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
