{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d201570-1d34-45a8-8879-b54ca3bc8fcf",
   "metadata": {},
   "source": [
    "# Broadcast Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13988d29-ff9b-4590-ba9e-c30f6b21e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b71ab3-1148-4509-a006-ecc765faa376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_add(shape1, shape2):\n",
    "    assert len(shape1) == 2 and len(shape2) == 2, \"Only 2 Dimension is supported\"\n",
    "\n",
    "    for i in range(len(shape1)):\n",
    "        assert shape1[i]==shape2[i] or shape1[i] == 1 or shape2[i] == 1, \"Shape doesn't fit broadcasting shape\"\n",
    "\n",
    "    A = te.placeholder(shape1, name='A')\n",
    "    B = te.placeholder(shape2, name='B')\n",
    "    # (3, 1) * (3 * 4)\n",
    "    # m = 3, n = 4\n",
    "    m = shape1[0] if shape2[0] == 1 else shape2[0]\n",
    "    n = shape1[1] if shape2[1] == 1 else shape2[1]\n",
    "    f = lambda x, y: A[0 if shape1[0] == 1 else x, 0 if shape1[1]==1 else y] + B[0 if shape2[0] == 1 else x, 0 if shape2[1] == 1 else y]\n",
    "    C = te.compute((m,n),f, name='c')\n",
    "\n",
    "    return A, B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36fc1e1c-e15a-471b-b2b3-b10e60584098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [3], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [12], [])}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [3, 1], []), B_1: B_3: Buffer(B_2, float32, [3, 4], [])} {\n",
      "  allocate(c: Pointer(global float32), float32, [12]), storage_scope = global;\n",
      "  for (x: int32, 0, 3) {\n",
      "    for (y: int32, 0, 4) {\n",
      "      let cse_var_1: int32 = ((x*4) + y)\n",
      "      c_1: Buffer(c, float32, [12], [], align=32)[cse_var_1] = (A[x] + B[cse_var_1])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "n = 4\n",
    "shape1 = (m, 1)\n",
    "shape2 = (m, n)\n",
    "A, B, C = broadcast_add(shape1, shape2)\n",
    "s = te.create_schedule(C.op)\n",
    "print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "mod = tvm.build(s, [A, B, C])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192081e0-b17a-4e26-bbad-452d2704545d",
   "metadata": {},
   "source": [
    "# Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d35cb0d4-88fe-45b9-a111-1f4141b1619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(n, m, l):\n",
    "    \"\"\"Return the computing expression of matrix multiplication\n",
    "    A : n x l matrix\n",
    "    B : l x m matrix\n",
    "    C : n x m matrix with C = A B\n",
    "    \"\"\"\n",
    "\n",
    "    k = te.reduce_axis((0, l), name='k')\n",
    "    A = te.placeholder((n, l), name='A')\n",
    "    B = te.placeholder((l, m), name='B')\n",
    "    C = te.compute((n, m),\n",
    "                    lambda x, y: te.sum(A[x, k] * B[k, y], axis=k),\n",
    "                    name='C')\n",
    "    return A, B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7085251f-b57c-4332-8bc3-a77b3e37f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [10000], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [10000], [])}\n",
      "  buffer_map = {A_1: A, B_1: B}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [100, 100], []), B_1: B_3: Buffer(B_2, float32, [100, 100], [])} {\n",
      "  allocate(C: Pointer(global float32), float32, [10000]), storage_scope = global;\n",
      "  for (x: int32, 0, 100) {\n",
      "    for (y: int32, 0, 100) {\n",
      "      C_1: Buffer(C, float32, [10000], [])[((x*100) + y)] = 0f32\n",
      "      for (k: int32, 0, 100) {\n",
      "        let cse_var_2: int32 = (x*100)\n",
      "        let cse_var_1: int32 = (cse_var_2 + y)\n",
      "        C_1[cse_var_1] = (C_1[cse_var_1] + (A[(cse_var_2 + k)]*B[((k*100) + y)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A, B, C = matmul(n, n, n)\n",
    "s = te.create_schedule(C.op)\n",
    "print(tvm.lower(s, [A, B], simple_mode=True))\n",
    "mod = tvm.build(s, [A, B, C])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17777b-0a73-4e75-b487-f04b6de4cdb8",
   "metadata": {},
   "source": [
    "#### C\n",
    "\n",
    "C_1: Buffer(C, float32, [10000], [])[((x*100) + y)] = 0f32\n",
    "Total elements is 10000, hence that much buffer is allocated, and the stride is 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6a9a5-577c-46b0-b660-0c43040fc996",
   "metadata": {},
   "source": [
    "# Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14141f-4845-411f-b4e6-e81592e349dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04bc733-f79b-4cf2-ae1b-d5dc9e10fc1a",
   "metadata": {},
   "source": [
    "The convolution (CONV) operator is the one of the most expensive and popular operators in neural networks.\n",
    "\n",
    "#### Padding\n",
    "\n",
    "As a prerequisite to convolution, let’s first implement padding, which visually surrounds the targeting tensor with a “shell” surrounding it. The padding values are normally 0.\n",
    "\n",
    "if the matrix height (i.e. number of rows) is  𝑛ℎ\n",
    "  and width (i.e. number of columns) is  𝑛𝑤\n",
    " , then we will pad  𝑝ℎ\n",
    "  rows with 0s on top and bottom, and  𝑝𝑤\n",
    "  columns with 0s on left and right to make its height and width to  𝑛ℎ+2𝑝ℎ\n",
    "  and  𝑛𝑤+2𝑝𝑤\n",
    " , respectively.\n",
    "\n",
    "- we assume the last two dimensions are rows and columns, 0s are only padded on these two dimensions.\n",
    "\n",
    "```\n",
    "X.shape = (Sample_N, C, N, M)\n",
    "X.shape[-2] = N = Row (height)\n",
    "X.shape[-1] = M = Columns (width)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beae075a-6c6a-4d4c-aa8c-ec8904cf4340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(X, ph, pw, val=0):\n",
    "    \"\"\"Pad X with the given value in 2-D\n",
    "\n",
    "    ph, pw : height and width padding\n",
    "    val : padding value, default 0\n",
    "    \"\"\"\n",
    "    assert len(X.shape) >= 2\n",
    "\n",
    "    nh, nw = X.shape[-2], X.shape[-1]\n",
    "\n",
    "    return te.compute(\n",
    "        (*X.shape[0: -2], nh + ph * 2, nw + pw * 2),\n",
    "        lambda *i: te.if_then_else(\n",
    "            te.any(i[-2] < ph, i[-2]>=nh+ph, i[-1]<pw, i[-1]>=nw+pw), \n",
    "            val, X[i[:-2]+ (i[-2]-ph, i[-1]-pw)]), \n",
    "        name='PaddedX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460796f7-5106-4234-833f-44939878b63e",
   "metadata": {},
   "source": [
    "Final X value: i[:-2]+ (i[-2]-ph, i[-1]-pw)\n",
    "\n",
    "Here, we are appending the first 2 dimension of the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9da16709-11ce-4e52-9d1f-269171341435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 1. 1. 1. 0. 0.]\n",
      "  [0. 0. 1. 1. 1. 1. 0. 0.]\n",
      "  [0. 0. 1. 1. 1. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 1. 1. 1. 0. 0.]\n",
      "  [0. 0. 1. 1. 1. 1. 0. 0.]\n",
      "  [0. 0. 1. 1. 1. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "A = te.placeholder((2,3,4))\n",
    "B = padding(A, 1, 2)\n",
    "s = te.create_schedule(B.op)\n",
    "mod = tvm.build(s, [A, B])\n",
    "\n",
    "a = tvm.nd.array(np.ones((2,3,4), dtype='float32'))\n",
    "b = tvm.nd.array(np.empty((2,5,8), dtype='float32'))\n",
    "mod(a, b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e65e37-04d9-4fdf-b814-a2e5174c59ef",
   "metadata": {},
   "source": [
    "#### General Convolution Formula\n",
    "\n",
    "⌊(𝑛ℎ−𝑘ℎ+2𝑝ℎ)/𝑠ℎ+1⌋×⌊(𝑛𝑤−𝑘𝑤+2𝑝𝑤)/𝑠𝑤+1⌋\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f113a5e9-d174-4fe2-a6fb-1874672df59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_out_size(n, k, p, s):\n",
    "    return (n - k + 2 * p)//s + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5e64d86-d6ee-4045-95d5-43d48cfcd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(oc, ic, nh, nw, kh, kw, ph=0, pw=0, sh=1, sw=1):\n",
    "    \"\"\"Convolution\n",
    "\n",
    "    oc, ic : output and input channels\n",
    "    nh, nw : input width and height\n",
    "    kh, kw : kernel width and height\n",
    "    ph, pw : height and width padding sizes, default 0\n",
    "    sh, sw : height and width strides, default 1\n",
    "    \"\"\"\n",
    "\n",
    "    #reduction axes (we reduce in a all dimension when we multiple with the kernel to produce 1 value)\n",
    "    ric = te.reduce_axis((0, ic), name='ric')\n",
    "    rkh = te.reduce_axis((0, kh), name='rkh')\n",
    "    rkw = te.reduce_axis((0, kw), name='rkw')\n",
    "\n",
    "    # output height and width\n",
    "    oh = conv_out_size(nh, kh, ph, sh)\n",
    "    ow = conv_out_size(nw, kw, pw, sw)\n",
    "\n",
    "    # Padding\n",
    "    X = te.placeholder((ic, nh, nw), name='X')\n",
    "    K = te.placeholder((oc, ic, kh, kw), name='K')\n",
    "    PaddedX = padding(X, ph, pw) if ph * pw != 0 else X\n",
    "\n",
    "    Y = te.compute(\n",
    "        (oc, oh, ow),\n",
    "        lambda c, i, j: te.sum(PaddedX[ric, i*sh+rkh, j*sw+rkw] * K[c, ric, rkh, rkw], axis=[ric, rkh, rkw]), name='Y'\n",
    "    )\n",
    "\n",
    "    return X, K, Y, PaddedX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e7f1557-7d9e-469d-a582-f200a1519e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_data(oc, ic, n, k, p=0, s=1, constructor=None):\n",
    "    \"\"\"Return random 3-D data tensor, 3-D kernel tenor and empty 3-D output\n",
    "    tensor with the shapes specified by input arguments.\n",
    "\n",
    "    oc, ic : output and input channels\n",
    "    n : input width and height\n",
    "    k : kernel width and height\n",
    "    p : padding size, default 0\n",
    "    s : stride, default 1\n",
    "    constructor : user-defined tensor constructor\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    data = np.random.normal(size=(ic, n, n)).astype('float32')\n",
    "    weight = np.random.normal(size=(oc, ic, k, k)).astype('float32')\n",
    "    on = conv_out_size(n, k, p, s)\n",
    "    out = np.empty((oc, on, on), dtype='float32')\n",
    "    if constructor:\n",
    "        data, weight, out = (constructor(x) for x in [data, weight, out])\n",
    "    return data, weight, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e7f6932-d6cc-4117-ba60-c55123457322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(X_1: handle, K_1: handle, Y_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {X: Buffer(X_2: Pointer(float32), float32, [864], []),\n",
      "             K: Buffer(K_2: Pointer(float32), float32, [216], []),\n",
      "             Y: Buffer(Y_2: Pointer(float32), float32, [576], [])}\n",
      "  buffer_map = {X_1: X, K_1: K, Y_1: Y}\n",
      "  preflattened_buffer_map = {X_1: X_3: Buffer(X_2, float32, [6, 12, 12], []), K_1: K_3: Buffer(K_2, float32, [4, 6, 3, 3], []), Y_1: Y_3: Buffer(Y_2, float32, [4, 12, 12], [])} {\n",
      "  allocate(PaddedX: Pointer(global float32), float32, [1176]), storage_scope = global {\n",
      "    for (i0: int32, 0, 6) {\n",
      "      for (i1: int32, 0, 14) {\n",
      "        for (i2: int32, 0, 14) {\n",
      "          PaddedX_1: Buffer(PaddedX, float32, [1176], [])[(((i0*196) + (i1*14)) + i2)] = @tir.if_then_else(((((i1 < 1) || (13 <= i1)) || (i2 < 1)) || (13 <= i2)), 0f32, X[((((i0*144) + (i1*12)) + i2) - 13)], dtype=float32)\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (c: int32, 0, 4) {\n",
      "      for (i: int32, 0, 12) {\n",
      "        for (j: int32, 0, 12) {\n",
      "          Y[(((c*144) + (i*12)) + j)] = 0f32\n",
      "          for (ric: int32, 0, 6) {\n",
      "            for (rkh: int32, 0, 3) {\n",
      "              for (rkw: int32, 0, 3) {\n",
      "                let cse_var_1: int32 = (((c*144) + (i*12)) + j)\n",
      "                Y[cse_var_1] = (Y[cse_var_1] + (PaddedX_1[(((((ric*196) + (i*14)) + (rkh*14)) + j) + rkw)]*K[((((c*54) + (ric*9)) + (rkh*3)) + rkw)]))\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schedule a conv function with below parameters\n",
    "oc, ic, n, k, p, s = 4, 6, 12, 3, 1, 1\n",
    "X, K, Y, _ = conv(oc, ic, n, n, k, k, p, p, s, s)\n",
    "\n",
    "sch = te.create_schedule(Y.op)\n",
    "mod = tvm.build(sch, [X, K, Y])\n",
    "print(tvm.lower(sch, [X, K, Y], simple_mode=True))\n",
    "\n",
    "data, weight, out = get_conv_data(oc, ic, n, k, p, s, tvm.nd.array)\n",
    "mod(data, weight, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64f443d2-6d8c-40e5-982f-19fc61b6d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "def get_conv_data_mxnet(oc, ic, n, k, p, s, ctx='cpu'):\n",
    "    ctx = getattr(mx, ctx)()\n",
    "    data, weight, out = get_conv_data(oc, ic, n, k, p, s,\n",
    "                                      lambda x: mx.nd.array(x, ctx=ctx))\n",
    "    data, out = data.expand_dims(axis=0), out.expand_dims(axis=0)\n",
    "    bias = mx.nd.zeros(out.shape[1], ctx=ctx)\n",
    "    return data, weight, bias, out\n",
    "\n",
    "# Save to the d2ltvm package.\n",
    "def conv_mxnet(data, weight, bias, out, k, p, s):\n",
    "    mx.nd.Convolution(data, weight, bias, kernel=(k,k), stride=(s,s),\n",
    "                      pad=(p,p), num_filter=out.shape[1], out=out)\n",
    "\n",
    "data, weight, bias, out_mx = get_conv_data_mxnet(oc, ic, n, k, p, s)\n",
    "conv_mxnet(data, weight, bias, out_mx, k, p, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7ef574c-b3b2-4974-a7bc-83d95a49ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(out_mx[0].asnumpy(), out.asnumpy(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477e502-0201-4ff1-b05b-c4581e64216b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
